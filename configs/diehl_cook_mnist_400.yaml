# RTX 3080 Optimized Configuration for Diehl & Cook (BindsNET)
# Tuned for batch_eth_mnist.py and thesis-grade experiments
# Expected performance: 15-25 minutes training, 92-94% accuracy

model:
  name: "DiehlAndCook2015"
  n_neurons: 400          # Excitatory neurons per network (sweet spot for RTX 3080)
  n_inpt: 784
  exc: 22.5
  inh: 120.0              # Inhibition strength (w_inh) - keeps competition sharp with fewer timesteps
  theta_plus: 0.05
  dt: 1.0                 # ms timestep
  norm: 78.4
  nu: [0.0001, 0.01]      # Learning rates [min, max] - faster convergence at shorter times
  inpt_shape: [1, 28, 28]
  w_dtype: "float32"
  sparse: true
  inh_thresh: -52.0
  exc_thresh: -40.0

training:
  batch_size: 32         # Parallel networks - fully utilizes RTX 3080 without VRAM pressure
  n_epochs: 1
  n_train: 60000          # Standard for Diehl-Cook, converges reliably (NOT full 60k)
  n_updates: 10
  update_interval: 250    # Update assignments every 250 samples
  time: 150               # ms per sample (cuts runtime by ~2.3x vs 350ms default)
  intensity: 128.0        # Stronger Poisson drive → earlier spikes → less wasted simulation
  progress_interval: 10
  checkpoint_interval: 1
  seed: 0
  gpu: true
  
  # Optimization flags (disabled by default for speed)
  enable_monitoring: false      # Disable monitors during training (1.5-2x speedup)
  enable_grad_tracking: false   # Disable gradient tracking (STDP doesn't need it)
  enable_weight_norm: false      # Disable weight normalization during training (normalize only at update_interval)

inference:
  batch_size: 32
  n_test: 10000
  time: 150
  intensity: 128.0
  gpu: true
  enable_monitoring: true       # Enable monitoring for inference/analysis

data:
  dataset: "MNIST"
  root: null  # Will use bindsnet default
  n_workers: 8

paths:
  checkpoint_dir: "checkpoints"
  model_dir: "models"
  log_dir: "logs"
  data_dir: "data"

